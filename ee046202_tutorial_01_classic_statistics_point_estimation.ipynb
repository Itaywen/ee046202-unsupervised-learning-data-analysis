{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "\n",
    "* Formerly 046193 \n",
    "\n",
    "#### Tal Daniel\n",
    "\n",
    "## Tutorial 01 - Classical Methods in Statistical Inference - Point Estimation\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "\n",
    "* Point Estimation\n",
    "* Evaluating Estimators\n",
    "    * Bias-Variance of Estimators\n",
    "* Maximum Likelihood Estimator (MLE)\n",
    "    * KL Divergence and Asymptotic Consequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/doodle/48/000000/statistics.png\" style=\"height:50px;display:inline\"> Statistical Inference - Classical Methods\n",
    "* **Statistical Inference** - a collection of methods and tools to draw conclusions from data that are usually affected by randomness.\n",
    "    * General Setup - there is an unknown quantity that we wish to estimate by observing given data. There are 2 approaches:\n",
    "        * Classical Inference (**Frequentist**) - the unknown quantity $\\theta$ is deterministic. We estimate non-random quantities.\n",
    "        * Bayesian Inference - we assume the unknown quantity $\\theta$ is a random variable and we make assumptions on the type of distribution. After observing the data, we can update the distribution using Bayes' rule.  We estimate random variables.\n",
    "* Examples:\n",
    "    * Predicting the results of an election - we cannot possibly poll the entire population, thus, we pick a random sample from the population to get \"where the wind blows\". Here, the randomness come from the sampling process. Another source of randomness may be the time frame in which the poll was conducted (one month or one week before the elections).\n",
    "        * In the classical approach: $\\theta$ is the percentage of people that vote for candidate A. After polling $n$ randomly chosen voters, where $n_A$ voters said they would vote for candidate A, we may estimate $\\theta$ by: $$ \\hat{\\theta} = \\frac{n_A}{n} $$\n",
    "            * Notice that $\\hat{\\theta}$ is a **random variable** as it depends on the *random* sample. \n",
    "    * Receiver-Transimitter - the receiver may get a corrupted version of messages due to random noise.\n",
    "        * In the Bayesian approach, $\\theta \\sim Bernoulli(p)$ is the transimitted bit ({0,1}), where $p$ is the proabibility to transmit 1. The receiver has to recover $\\theta$ based on the knowledge of the distribution.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/cotton/64/000000/pickup-point.png\" style=\"height:50px;display:inline\"> Point Estimation\n",
    "* **Assumption** - $\\theta$ is fixed, non-random, quantity.\n",
    "    * Example: $\\theta$ can be the expected value of a random variable $\\theta = \\mathbb{E}[x]$\n",
    "* The data - a random sample $\\{X_i\\}_{i=1}^n$ such that $X_i$'s have the **same distribution** as $X$.\n",
    "* The point estimator - a function of the random sample: $$ \\hat{\\theta} = h(X_1, X_2, ..., X_n) $$\n",
    "    * Note: the estimator may depend *stochastically* on the data, but we will assume the dependence is deterministic.\n",
    "    * Example: if $\\theta = \\mathbb{E}[x]$ then we may choose $\\hat{\\theta}$ to be: $$ \\hat{\\theta} = \\overline{X} = \\frac{X_1 + X_2 + ... + X_n}{n} $$\n",
    "        * Note: this not always the 'best' estimator for the mean\n",
    "* There are many possible estimators for $\\theta$, so how can we make sure we have chosen a good estimator? We need to define ways to evaluate our estimators.\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/cotton/64/000000/rating.png\" style=\"height:30px;display:inline\"> Evaluating Estimators\n",
    "* **<a style=\"color:red\"> Bias </a>** - the bias of an estimator $B(\\hat{\\theta})$ is a measure of how far is the estimator $\\hat{\\theta}$ from the real $\\theta$ **on average**. Formal definition: $$ B(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta $$\n",
    "    * Note that the bias cannot be actually computed (why is that?).\n",
    "    * Recall that $\\hat{\\theta}$ is a **random variable**.\n",
    "    * **Unbiased** estimator - we would like the bias to be close to 0, which indicates that on average $\\hat{\\theta}$ is close to $\\theta$. $\\hat{\\theta}$ is an *unbiased* estimator of $\\theta$ if: $B(\\hat{\\theta}) = 0 \\rightarrow \\mathbb{E}[\\hat{\\theta}] = \\theta$\n",
    "    \n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - Bias of an Estimator\n",
    "Let $X_1, X_2, ..., X_n$ be a random sample. Assume (always) that the samples are independent and identically distributed (iid).\n",
    "1. Show that the sample mean: $\\hat{\\theta}$ to be: $ \\hat{\\theta} = \\overline{X} = \\frac{X_1 + X_2 + ... + X_n}{n}$ is an *unbiased* estimator of $\\theta=\\mathbb{E}[X_i]$\n",
    "2. If we choose $ \\hat{\\theta}_1 = X_1 $, is the the estimator unbiased? Is it a good estimator?\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "\n",
    "1. $B(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta = \\mathbb{E}[\\overline{X}] - \\theta =\\mathbb{E}[X_i] - \\theta = 0 $\n",
    "2. If we choose $ \\hat{\\theta}_1 = X_1 $ then $ B(\\hat{\\theta}_1) = \\mathbb{E}[\\hat{\\theta}_1] - \\theta = \\mathbb{E}[X_1] - \\theta = 0$. This is an *unbiased* estimation! But is it good, or is it as good as $\\hat{\\theta} = \\overline{X}$? Not necessarily, as there can be many samples with different values.\n",
    "\n",
    "\n",
    "* **<a style=\"color:red\"> MSE </a>** (Mean Squared Error) - the MSE of an estimator: $$ MSE(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\theta)^2 \\big] $$\n",
    "    * Note that the expression $\\hat{\\theta} - \\theta$ is the *error* we make by estimating $\\theta$ with $\\hat{\\theta}$.\n",
    "    * The MSE is a measure of the expected (squared) error. **Smaller** MSE is generally an indication of a better estimator.\n",
    "    * We define the **variance** of the estimator as follows: $Var(\\hat{\\theta}) = \\mathbb{E}\\big[(\\mathbb{E}[\\hat{\\theta}] - \\hat{\\theta})^2 \\big]$\n",
    "    * It holds: (HW) $$ MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + Bias^2(\\hat{\\theta}) $$\n",
    "    \n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - MSE of an Estimator\n",
    "Let $X_1, X_2, ..., X_n$ be a random sample with mean $\\mathbb{E}[X_i] = \\theta$ and variance $Var(X_i) = \\sigma^2$\n",
    " and consider the following estimators:\n",
    " 1. $\\hat{\\theta}_1 = X_1$\n",
    " 2. $\\hat{\\theta}_2 = \\overline{X} = \\frac{X_1 + X_2 + ... + X_n}{n}$\n",
    " \n",
    "Find $MSE(\\hat{\\theta}_1), MSE(\\hat{\\theta}_2)$ and show that for $n>1$ we have $MSE(\\hat{\\theta}_1) > MSE(\\hat{\\theta}_2)$.\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "\n",
    "$$ MSE(\\hat{\\theta}_1) = \\mathbb{E}\\big[(\\hat{\\theta}_1 - \\theta)^2 \\big] = \\mathbb{E}\\big[(X_1 - \\mathbb{E}[X_1])^2 \\big] = Var(X_1) = \\sigma^2 $$ \n",
    "\n",
    "* We use the following:\n",
    "    * $\\mathbb{E}[X^2] = Var(X) + (\\mathbb{E}[X])^2 $\n",
    "    * For a constant $b$: $Var(X + b) = Var(X)$\n",
    "    * $\\mathbb{E}[\\overline{X} - \\theta] = \\frac{n\\mathbb{E}[X_i]}{n} - \\mathbb{E}[X_i] = 0$\n",
    "    * $Var[\\overline{X}] = Var[\\frac{1}{n}\\sum_{i=1}^nX_i] = \\frac{1}{n^2}n\\sigma^2 = \\frac{\\sigma^2}{n}$\n",
    "        * Recall that the samples are i.i.d., and the variance of the sum of i.i.d. samples is the sum of the variances.\n",
    "\n",
    "$$ MSE(\\hat{\\theta}_2) = \\mathbb{E}\\big[(\\hat{\\theta}_2 - \\theta)^2 \\big] = \\mathbb{E}\\big[(\\overline{X} - \\theta)^2 \\big] = Var(\\overline{X} - \\theta) + (\\mathbb{E}[\\overline{X} - \\theta])^2 = Var(\\overline{X}) + 0 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$ \n",
    "        \n",
    "\n",
    "Thus, for $n>1$: $MSE(\\hat{\\theta}_1) > MSE(\\hat{\\theta}_2) $, which means that $\\overline{X}$ is a better estimator.\n",
    "\n",
    "* **<a style=\"color:red\"> Consistency </a>** - an estimator is consistent if as the sample size $n$ grows, then $\\hat{\\theta}$ converges to the real value of $\\theta$.\n",
    "    * Formally: Let $\\hat{\\theta}_1, \\hat{\\theta}_2, ..., \\hat{\\theta}_n$ be a sequence of point estimators of $\\theta$. We say that $\\hat{\\theta}_n$ is a **consistent** estimator of $\\theta$, if: $$ \\lim_{n \\to \\infty} P(|\\hat{\\theta}_n - \\theta| \\geq \\epsilon) = 0, \\forall \\epsilon >0 $$\n",
    "    * Note: other convergence types yield different definitions of consistency\n",
    "    \n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - Consistency of an Estimator\n",
    "Let $X_1, X_2, ..., X_n$ be a random sample with mean $\\mathbb{E}[X_i] = \\theta$ and variance $Var(X_i) = \\sigma^2$.\n",
    "\n",
    "Show that $\\hat{\\theta}_n = \\overline{X}$ is a *consistent* estimator of $\\theta$.\n",
    "\n",
    "Reminder:\n",
    "* **Chebyshev's inequality**: let $\\mu = \\mathbb{E}[X], \\sigma^2= Var[X]$. Then: $ P(|X- \\mu| > t) \\leq \\frac{\\sigma^2}{t^2}$\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "\n",
    "$$ P(|\\hat{\\theta}_n - \\theta| \\geq \\epsilon) = P(|\\overline{X} - \\theta| \\geq \\epsilon) $$\n",
    "\n",
    "* Using **Chebyshev's inequality** and recall that $\\overline{X}$ is a random variable with **mean** $\\theta$:\n",
    "\n",
    "$$ \\rightarrow P(|\\overline{X} - \\theta| \\geq \\epsilon) \\leq \\frac{Var[{\\overline{X}]}}{\\epsilon^2} = \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\epsilon^2} $$\n",
    "$$ \\rightarrow \\lim_{n \\to \\infty} \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\epsilon^2} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/75-percents.png\" style=\"height:50px;display:inline\"> Point Estimation of Mean and Variance \n",
    "The sample mean, $\\overline{X}$, is often a reasonable point estimator for the mean. But what about the variance? Before we assumed the the variance was known, but when we want to estimate it, we need to take a similar approach.\n",
    "\n",
    "By definition, the variance of a distribution $\\sigma^2$ is: $$ \\sigma^2 = \\mathbb{E}[(X - \\mu)^2] $$.\n",
    "* If we define the following *random variable*: $Y = (X -\\mu)^2$ then the number $\\sigma^2$ is the mean of that variable, that is $\\sigma^2 = \\mathbb{E}[Y]$.\n",
    "* But wait, if $\\sigma^2$ is the mean of $Y$, we have already derived a point estimator for the mean! $$ \\hat{\\sigma}^2 = \\hat{Y} = \\frac{1}{n}\\sum_{i=1}^nY_i = \\frac{1}{n}\\sum_{i=1}^n (X_i -\\mu)^2 $$\n",
    "    * By the **linearity** of the expectation, this is an **unbiased** estimator of the variance.\n",
    "    * From the **weak law of large numbers** this is also a **consistent** estimator of the variance.\n",
    "* The problem? What if do not know the value of $\\mu$? It is often reasonable to replace $\\mu$ with our point estimate of $\\mu$, which transforms the estimate for $\\sigma^2$ to: $$ \\overline{S}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 =... =\\frac{1}{n}\\big( \\sum_{i=1}^nX_i^2 -n\\overline{X}^2\\big)  $$\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - Bias of the Variance Estimator\n",
    "Let $X_1, X_2, ..., X_n$ be a random sample with mean $\\mathbb{E}[X_i] = \\mu$ and variance $Var(X_i) = \\sigma^2$.\n",
    "\n",
    "Suppose that we use: $$ \\overline{S}^2 = \\frac{1}{n}\\big( \\sum_{i=1}^nX_i^2 -n\\overline{X}^2\\big) $$ as our estimate for $\\sigma^2$. Find the bias of the estimator: $$ B[\\overline{S}^2] = \\mathbb{E}[\\overline{S}^2] -\\sigma^2 $$\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "* $\\mathbb{E}[\\overline{X}^2] = (\\mathbb{E}[\\overline{X}])^2 + Var(\\overline{X}) = \\mu^2 +\\frac{\\sigma^2}{n}$\n",
    "$$ \\mathbb{E}[\\overline{S}^2] = \\frac{1}{n}\\big(\\sum_{i=1}^n\\mathbb{E}[X_i^2] -n \\mathbb{E}[\\overline{X}^2] \\big) $$ $$ =\\frac{1}{n} \\big( n(\\mu^2 +\\sigma^2) -n(\\mu +\\frac{\\sigma^2}{n})\\big) = \\frac{n-1}{n} \\sigma^2 $$ $$ \\rightarrow B[\\overline{S}^2] = \\mathbb{E}[\\overline{S}^2] -\\sigma^2 = -\\frac{\\sigma^2}{n} $$\n",
    "\n",
    "Thus, $ \\overline{S}^2$ is a **biased estimator** of the variance!\n",
    "* If $n$ is very *large*, then the bias is very **small**.\n",
    "* How can we obtain an **unbiased estimator** of the variance?\n",
    "    * By simply multiplying $\\overline{S}^2$ by $\\frac{n}{n-1}$.\n",
    "\n",
    "In conclusion, we define the *unbiased* estimator of the variance, called the **sample variance** to be: $$ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2 =... =\\frac{1}{n-1}\\big( \\sum_{i=1}^nX_i^2 -n\\overline{X}^2\\big) $$\n",
    "* The **sample standard deviation** is $S = \\sqrt{S^2}$, which is an **unbiased estimator** of the standard deviation.\n",
    "* Note: a good estimator should be **asymptotically unbiased**. In many cases, for a finite sample, a biased estimator is *better* than an unbiased one! (recall overfitting from ML?). There is nothing special in the lack of bias, *except* asymptotically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/ultraviolet/80/000000/tail-of-whale.png\" style=\"height:50px;display:inline\">Non-Parametric Point Estimation Using The Tail Sum Formula\n",
    "* Every random variable with possible values $\\{0,1,...,n\\}$ is a **counting variable** representing number of events that occur in some list of $n$ events $A_1,..., A_n$.\n",
    "* To see this, let $A_j$ be the event that $(X \\geq j)$. If $X=x$ for $0 \\leq x \\leq n$, then $A_j$ occurs for $1 \\leq j \\leq x$ and $A_j$ **does not** occur for $x<j\\leq n$. So if $X =x$, the number of events $A_j$ that occur is exactly $x$.\n",
    "* The resulting formula for $\\mathbb{E}[X]$ - **The Tail Sum Formula for Expectation**: $$ \\text{For } X \\text{ with possible values } \\{0,1...,n\\}, $$ $$ \\mathbb{E}[X] = \\sum_{j=1}^n P(X\\geq j) $$\n",
    "* Proof outline:\n",
    "    * Define $p_j = P(X=j)$.\n",
    "    * The expectation $\\mathbb{E}[X] = 1p_1 + 2p_2 + 3p_3 +... + np_n$ is the following sum: $$ p_1 $$ $$ +p_2 +p_2 $$ $$ +p_3 +p_3 + p_3 $$ $$ +...+... +... $$ $$ +p_n +p_n +... +p_n $$\n",
    "    * By the addition rule of probabilities, and the assumption that the only possible values of $X$ are $\\{0,1,...,n\\}$, the sum of the first column of $p$ is $P(X\\geq1)$, the sum of the second solumn is $P(X\\geq 2)$ and so on. The sum of the $j^{th}$ column is $P(X\\geq j), 1 \\leq j \\leq n$. The whole sum is the sum of the column sums: $$ \\sum_{j=1}^n P(X\\geq j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/maximum-value.png\" style=\"height:50px;display:inline\"> Maximum Likelihood Estimation (MLE)\n",
    "* The MLE is an estimator that picks the best parameters by maximizing the **likelihood** of the distribution. Recall that when we developed Bayes rule, the likelihood is $p(D|\\theta)$ (which is a function of $\\theta$).\n",
    "* Definition: $$\\hat{\\theta}_{MLE} = \\underset{\\theta \\in \\mathcal{R}^{p}}{\\mathrm{argmax}} p(D|\\theta) =  \\underset{\\theta \\in \\mathcal{R}^{p}}{\\mathrm{argmax}} \\log p(D|\\theta)$$\n",
    "    * The last equality is true since the log function is monotonically increasing. Therefore if a function $f(x) \\geq 0$, achieves a maximum at $x_1$, then $\\log⁡(f(x))$ also achieves a maximum at $x_1$\n",
    " \n",
    "* We assume the variables are **I.I.D (independent identically distributed)**. Note that these are the samples.\n",
    "* $L(\\theta) = p(D|\\theta) = p(x_1, x_2, ..., x_n|\\theta) = \\prod_{k=1}^n p(x_k|\\theta)$\n",
    "* $l(\\theta) = \\log \\big(L(\\theta)\\big) = \\sum_{k=1}^{n} \\log p(x_k|\\theta)$\n",
    "    * $\\log(x * y * z) = \\log x + \\log y + \\log z$\n",
    "* $\\rightarrow \\hat{\\theta}_{MLE} =  \\underset{\\theta \\in \\mathcal{R}^{p}}{\\mathrm{argmax}} \\{ l(\\theta)\\}$\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:50px;display:inline\"> Exercise  - MLE for Univariate Gaussian\n",
    "Given $\\{x_i\\}_{i=1}^n$ i.i.d samples of $X \\sim N(\\mu, \\sigma^2)$, what is the MLE?\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "The first thing to ask yourself is, **what are the parameters** in this problem? In our case, the parametrs are $\\theta = [\\mu, \\sigma^2]$, it is just a matter of notation.\n",
    "\n",
    "* $p(x_i) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{- \\frac{1}{2} \\frac{(x_i - \\mu)^2}{\\sigma^2}}$\n",
    "* $L(\\theta) = L(\\mu, \\sigma^2) = p(x_1, x_2, ..., x_n |\\mu, \\sigma^2) = \\prod_{i=1}^n p(x_i|\\theta) = \\frac{1}{(2\\pi \\sigma^2)^{\\frac{n}{2}}} e^{\\frac{-1}{2 \\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2}$\n",
    "* $l(\\theta) = \\log L(\\theta) = -n (\\log \\pi + \\frac{1}{2} \\log \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2$\n",
    "\n",
    "#### Find the optimal $\\theta$\n",
    "As usual, find the point where the deriviative w.r.t $\\theta$ is 0\n",
    "* $\\frac{\\partial l}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) = 0 \\rightarrow \\hat{\\mu}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n x_i$\n",
    "* $\\frac{\\partial l}{\\partial \\sigma^2} = - \\frac{n}{2 \\sigma^2} + \\frac{1}{\\sigma^4}\\sum_{i=1}^n (x_i - \\mu)^2 = 0 $\n",
    "    * Plug in $\\mu = \\hat{\\mu}_{MLE} \\rightarrow \\hat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu}_{MLE})^2 $\n",
    "* Summary: $$ \\hat{\\mu}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n x_i$$ $$\\hat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu}_{MLE})^2 $$\n",
    "\n",
    "* Do these look familiar? These are the **empirical** mean and variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu mle:  5.340300917859494\n",
      "var mle:  38.21984387083833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20663998c88>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAHgCAYAAABJrX+JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZwcdZ3/8ddnZjK5D8gxOWFCCLkI5BhyAk44E2CJuqggiKiYRQF1RTGu64qrIutvdVcUuVlABUQQiSQQzgkEcoeQO2EIIZmchJBjyDmZ7++P6umunkwmPUlXV3f1+/l49CP1raqu/kxlZt5T36r6ljnnEBERkWgpCLsAERERST8FvIiISAQp4EVERCJIAS8iIhJBCngREZEIUsCLiIhEUFHYBaRTp06dXGlpadq298knn9C6deu0bS/XaX8kaF8k0/5I0L5Ipv2RLN37Y8GCBducc50bWhapgC8tLWX+/Plp215FRQXl5eVp216u0/5I0L5Ipv2RoH2RTPsjWbr3h5l9cKRl6qIXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyI5r3TyVEonTw27DJGsooAXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyIiEkGBBryZjTezVWZWaWaTG1huZnZnbPliMxvmW/avZrbMzJaa2eNm1iLIWkVERKIksIA3s0LgLmACMBC4yswG1lttAtA39poE3B17bw/gW0CZc+50oBC4MqhaRUREoibII/gRQKVzbo1z7gDwBDCx3joTgUedZzbQwcy6xZYVAS3NrAhoBWwMsFYREZFIMedcMBs2uwIY75y7Ptb+EjDSOXeTb53ngDucczNj7VeAHzjn5pvZt4FfAHuBF51zVx/hcybhHf1TUlIy/Iknnkjb11BdXU2bNm3Str1cp/2RoH2RLOz9sWTDTgAG92gfWg11wt4X2Ub7I1m698e4ceMWOOfKGlpWlLZPOZw1MK/+XxMNrmNmJ+Ad3fcGdgB/NbNrnHN/Omxl5+4D7gMoKytz5eXlx1W0X0VFBencXq7T/kjQvkgW9v64bvJUANZeHV4NdcLeF9lG+yNZJvdHkF30VUAvX7snh3ezH2mdC4D3nXMfOucOAn8DxgRYq4iISKQEGfDzgL5m1tvMivEukptSb50pwLWxq+lHATudc5uAdcAoM2tlZgacD6wIsFYREZFICayL3jlXY2Y3AdPxroJ/yDm3zMxuiC2/B5gGXAJUAnuAr8SWzTGzp4CFQA3wNrFueBERETm6IM/B45ybhhfi/nn3+KYdcOMR3vsT4CdB1iciIhJVGslOREQkghTwIiIiEaSAFxERiSAFvIiISAQp4EVERCJIAS8iIhJBCngREZEIUsCLiIhEUKAD3YiIpFNp7KEyAGvvuDTESkSyn47gRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIkgBLyIiEkEKeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRpIAXERGJIAW8iIhIBCngRUREIqgo7AJERBpSOnlqfHrtHZce8/uP5b0iUaAjeBERkQhSwIuIiESQAl5ERCSCFPAiIiIRFGjAm9l4M1tlZpVmNrmB5WZmd8aWLzazYbH5/cxske+1y8y+E2StIiIiURLYVfRmVgjcBVwIVAHzzGyKc265b7UJQN/YayRwNzDSObcKGOLbzgbgmaBqFRERiZogj+BHAJXOuTXOuQPAE8DEeutMBB51ntlABzPrVm+d84H3nHMfBFiriIhIpAQZ8D2A9b52VWxeU9e5Eng87dWJiIhEmDnngtmw2eeAi51z18faXwJGOOdu9q0zFfilc25mrP0KcKtzbkGsXQxsBAY557Yc4XMmAZMASkpKhj/xxBNp+xqqq6tp06ZN2raX67Q/ErQvkgWxP5Zs2BmfHtyj/RHn+ecfbV4m6HsjmfZHsnTvj3Hjxi1wzpU1tCzIkeyqgF6+dk+8sG7KOhOAhUcKdwDn3H3AfQBlZWWuvLz8OEpOVlFRQTq3l+u0PxK0L5IFsT+u849kd3X5Eef55x9tXiboeyOZ9keyTO6PILvo5wF9zax37Ej8SmBKvXWmANfGrqYfBex0zm3yLb8Kdc+LiIg0WWBH8M65GjO7CZgOFAIPOeeWmdkNseX3ANOAS4BKYA/wlbr3m1krvCvw/yWoGkVERKIq0IfNOOem4YW4f949vmkH3HiE9+4BOgZZn4iISFRpJDsREZEIUsCLiIhEkAJeREQkghTwIiIiEaSAFxERiSAFvIiISAQp4EVERCJIAS8iIhJBCngREZEIUsCLiIhEkAJeREQkghTwIiIiERTow2ZERNKh1Pcc+MbmHc+2195xaVq2J5ItdAQvIiISQQp4ERGRCFLAi4iIRJACXkREJIIU8CIiIhGkgBcREYkgBbyIiEgE6T54Ecl5RYdqOPf9hfCt6bBmDTRrxvc3N+OVPiPAXQJmYZcoknEKeBHJaRevfot/f/VBeu3ckjT/RuDG2X+F956Gu+6CYcPCKVAkJOqiF5Gc1OzQQX4+/S7ufeb2w8I9yezZMGIE/P73mStOJAso4EUk5zQ7dJC7n7mdaxY9H5+3vWU7+P734emn4bHH+MvgC9lf2MxbeOgQ3Hwz/OhHIVUsknnqoheRnGKulv957jdc8N68+Lx/9D+HH118I4t/9YX4vB+80467R11BxaIHYF5s3dtvhxNOgO99L9Nli2ScAl5Ecso3Z/2Vy1a+EW//YdQV/OrcLzd4Id3aE3tARQV8/vMwNfZwmu9/ny/P3cOMU4ZnqGKRcKiLXkRyxugP3uG7M/8cbz8y7NIjhntcq1bw17/COefEZ/3Pc7+m665tQZYqEjoFvIjkhFYH9vL/pv2WQlcLwJxep/Of509K7Ra4li3hqaege3cATty7izte+B04F2TJIqFSwItITrjljT/Rc9dWwLug7qbLb+VQQWHqG+jSBR5/PP4HQfn7C/inFa8HUapIVlDAi0jWG7TlPb4yf0q8/bPzrufDNic2fUPnngs33hhv/scr99N2/yfpKFEk6yjgRSTrTX7t/yjA605/vXQozwwad+wbu/12NrbtBEDnPTv4lzlPp6NEkayjgBeRrHbO+ws554NFAByyAn6a6nn3I2nblv/61Jfjza/Ne5aS3brgTqJHAS8iWctcLZMrHo63/3LGhbzXqddxb3fKwE+xtKQPAC1r9vPtN5847m2KZBsFvIhkrXHvzWfQ1jUA7C1qzv+O/WJatuusgDs+dV28fcWSl6GqKi3bFskWCngRyVrfmP1UfPpPQyewtW3HtG17ZukQ5vUYCEBxbQ385jdp27ZINlDAi0hWKqtaxlkblgNwoKCIB876dHo/wIw/jP5con3vvbBN5+IlOhTwIpKV/Efvfzv9PLbErnxPp9dOKWNF51KvsWcP/OEPaf8MkbAo4EUk+7z3HufHHiZTi3HfiM8G8zlm3D2q3lH8wYPBfJZIhingRST73HNPfPK1PmWs6dgzsI96vt8YPmzdwWts3AjPPhvYZ4lkkgJeRLLL3r3w0EPx5p+GXhLoxx0sbMZjZ05IzPj97wP9PJFMCTTgzWy8ma0ys0ozm9zAcjOzO2PLF5vZMN+yDmb2lJmtNLMVZjY6yFpFJEv89a+wfTsA69uXMKP3sKO84fg9NuRiKIyNaz9jBixdGvhnigQtsIA3s0LgLmACMBC4yswG1lttAtA39poE3O1b9lvgBedcf+BMYEVQtYpIFrk78WvgsSHjqW3KA2WO0Za2neAzn0nMePDBwD9TJGhBHsGPACqdc2uccweAJ4CJ9daZCDzqPLOBDmbWzczaAecCDwI45w4453YEWKuIZIOVK2H2bMC7Ne7JwRdm7rO//vXE9J//rIvtJOcFGfA9gPW+dlVsXirrnAJ8CPyfmb1tZg+YWesAaxWRbPDHP8YnXzl1BB/VXfyWCeefDz1iv6I+/BCefz5zny0SgKIAt93Q0yBciusUAcOAm51zc8zst8Bk4MeHfYjZJLzufUpKSqioqDiempNUV1endXu5TvsjQfsiWVr2R20tox58kBZ1zUvO5ZbBNU3ahL+Guvc2VFdD26144w16f+pTnPzYYwB8+N//zbJ27Zr0+aDvjfq0P5Jlcn8EGfBVgP+pED2BjSmu44Aq59yc2Pyn8AL+MM65+4D7AMrKylx5eflxF16noqKCdG4v12l/JGhfJEvL/pgxA7Zs8aZPPJHvFI3k4JKm/Ypae3WihusmTz1sXv1lh723a1eIBXzn2bMpHzwYOjZteFx9byTT/kiWyf0RZBf9PKCvmfU2s2LgSmBKvXWmANfGrqYfBex0zm1yzm0G1ptZv9h65wPLA6xVRML26KOJ6Suv5GBhs8zX0L8/jBzpTR88CE/oKXOSuwILeOdcDXATMB3vCvgnnXPLzOwGM7shtto0YA1QCdwPfNO3iZuBP5vZYmAIcHtQtYpIyPbuhacSQ9Ny7bXh1eL/7L/8Jbw6RI5TkF30OOem4YW4f949vmkH3HiE9y4CyoKsT0SyxPPPw65d3nTfvjBiBDwzrfH3BOWKK+Dmm6G2FmbOhA0bEhffieQQjWQnIuHzH71feSVYQ9ffZkiXLjBunDftHDz9dHi1iBwHBbyIhGvfPvjHPxLtK64Ir5Y6n/98YvrJJ8OrQ+Q4BNpFLyL5o9R3ZfraOy5N/Y0vvgjV1QCsOaE7pwwenO7S4kobuHq+QZ/5DHzzm3DoELz5JlRVQc/DH3hzzF+zSAboCF5EwuXrnn+h35hwu+frdO4M552XaPtPIYjkCAW8iIRn/36Ykrh7dlq/s0Msph5/N70CXnKQAl5EwvPKK7BzJ+A9OW5pSZ+QC/L59KehIPYr8q23vOFrRXKIAl5EwvO3v8Unp/Ubmx3d83U6dYIxY7xp52BqiufvRbKEAl5EwlFbC889F29O7zs6xGKO4PLLE9NT6g/EKZLdFPAiEo4FC+Jjz3/Ush2Lup8WckEN8Af89OneLX0iOUIBLyLh8B29V/Qpo7agMMRijqBfP+8FsGePd82ASI5QwItIOHwB/0qfESEWchTqppccpYAXkczbsAEWLvSmi4p4o/fQcOtpjD/g//EP79oBkRyggBeRzJvme5DMueeyu3nr8Go5mtGjE8+E37Qp8YeJSJZTwItI5vm657nssvDqSEVhIUyYkGhPnx5eLSJNoIAXkczauxdefjnRzvaABxg/PjH9wgvh1SHSBHrYjIjE1T085XgfnNLQQ13i25wxw7siHeC007znv7P6uD6vqbUcbb3Dvv6LLvIG4XEOZs2CHTugQ4eUt6kH0UgYdAQvIpn14ouJaX/Xdzbr3BmGD/emDx3S7XKSExTwIpJZL72UmL744vDqaCp100uOUcCLSOZs3AhLl3rTxcVw7rnh1tMU/oCfPt3rrhfJYgp4Eckc/9H72LHQOotvj6tv5Eho396bXr8eVqwItx6Ro1DAi0jm+AP+oovCq+NYFBXBhRcm2uqmlyyngBeRzKitze2Ah+Ru+uefD68OkRQo4EUkMxYvhq1bvelOnWDIkHDrORb+iwJnzqR5zYHwahE5CgW8iGSG/+j9ggugIAd//fTs6d27D7BvH8M2rAy3HpFG5OBPmIjkJP/977nYPV/n/PPjk2M+eCfEQkQap4AXkcA1P7gf3ngjMcN/sVquOe+8+KQCXrKZAl5EAjeiahns3+81Bgzwurpz1bhx3rC1wJmbVtNm/56QCxJpmAJeRAI3et3iROOCC8IrJB06doxfIFjkahmxfmnIBYk0TAEvIoEbtW5JfPpfqtpSOnlqyg+BCcNR61M3veQABbyIBKr1/j2cseldAGox5vQ6PeSK0sB3od1YBbxkqaM+LtbMCoAzge7AXmCZc25L0IWJSDSUbVhBkasFYGWXUna0bBdyRWlwzjneyHY1NQz4cC0dP9kRdkUihzliwJtZH+AHwAXAu8CHQAvgNDPbA9wLPOJc7CdXRKQB/vPvs3sNDrGSNGrTxhub/s03geRTECLZorEu+p8DfwL6OOcuds5d45y7wjl3BnA50B74UiaKFJHc5Q+/WSefEWIlaaZueslyRwx459xVzrnXnTv8mYjOua3Ouf91zj0SbHkiksva7N/D6ZsrgQidf6/ju9Bu9DoFvGSfIwa8md3um87hUSlEJCxlVcvi59+Xl5zCrhZtQq4ojUaNYn9hMwB6f7zJe9a9SBZprIve99gk/ivoQkQkekb7uudnR+noHaB5c97u3i/R9o/UJ5IFdJuciAQmsuffY+b4LxqcMSO8QkQa0Nhtcl3M7LuA+abjnHO/CbQyEclpbfd/wulb3gPgkBUwr+egkCtKvzm9fF/T66+HV4hIAxoL+PuBtg1Mi4gc1Vnrl1EYO/++LGrn32Pe7t6fgwWFNKs9BMuWwbZtYZckEnfEgHfO/TSThYhItPjvf591UvS65wH2Frdgcde+DN8Yey78G2/ACSeEW5RITKPn4M1snJk9bWbLYq+nzKw81Y2b2XgzW2VmlWY2uYHlZmZ3xpYvNrNhvmVrzWyJmS0ys/lN+qpEJHQjfQ9hmX1SRAa4acBc/8WD6qaXLNLYbXKXAg8BzwFfBK4GpgEPmdklR9uwmRUCdwETgIHAVWY2sN5qE4C+sdck4O56y8c554Y458pS+3JEJBOO9jCW1vv3MGjLGsC7/31BjwGZKi0wdV9z/a+7qefhs/1BOxIdjZ2D/z7waeecfwSHuqPp3+GFfWNGAJXOuTUAZvYEMBFY7ltnIvBobDCd2WbWwcy6Oec2NfULEZHsMXTjqvj591WdT47k+fc6C3oO5JAVeF/vokUUVleHXZIIANbAQHXeArOVzrn+TV3mW+cKYLxz7vpY+0vASOfcTb51ngPucM7NjLVfAX7gnJtvZu8DHwMOuNc5d98RPmcS3tE/JSUlw5944olGv+CmqK6upk2b6P5iairtj4So7oslG3YCMLhH+yatV11dzfs7D8WXj3z6cUY88xcAFl94CTO+PKnB7dS9v257R5qXKn/dx/L+VLdXv8Yv/Pt36bLW67GY+5OfsKe8PGl5Q9s52j6Oiqj+rByrdO+PcePGLThSL3djR/CfHOOyOtbAvPp/TTS2zljn3EYz6wK8FPuj4rD+r1jw3wdQVlbmymM/WOlQUVFBOreX67Q/EqK6L66LdR2vvbq8SetVVFTw65mJXwuPLVgRn76/xSD+saThXzV177/O12Xd0LxU+es+lvenur36NbbqNJivxQK+6+rVnHTbbUnLG9rO0fZxVET1Z+VYZXJ/NBbwfcxsSgPzDTglhW1XAb187Z5A/bEcj7iOc67u361m9gxel7+uYBHJckWHahi6cVW8HcX73+ub22sQX5v/LADt39G49JIdGgv4iY0s++8Utj0P6GtmvYENwJV4F+v5TQFuip2fHwnsdM5tMrPWQIFzbnds+iLgP1P4TBEJ2aAt79GyZj8A69uXsLldp5ArCt5c3x8xbVetgk8+gdatQ6xIpPH74GeY2VCgD7DMObfiSOse4f01ZnYTMB0oBB5yzi0zsxtiy+/Bu1DvEqAS2AN8Jfb2EuAZM6ur8THn3AtN+spEJBRnVS2LT8/rWf/GmWj6uFV7GDQIli2j4NAhmDULLrgg7LIkzx0x4M3sP4BrgAXAr8zsl865+5uycefcNOpdbR8L9rppB9zYwPvWAGc25bNEJDucVZW4UWZ+ngQ8AOee641mB/Dmmwp4CV1jA918ARjinLsKOIvYleoiIkfkHGW+gJ+bB+ff48aOTUy/+WZ4dYjENBbw+5xzewCccx8dZV0REfpsr6Lj3l0AfNyiLe917BlyRRk0ZkxievZsOHToyOuKZECqV9FbvTbOucsDrUxEck5ZUvf8AJzl0XFBaSl06wabNsHu3bBkyVHfIhKkplxFn8qV8yKSx/zn3/Ph9rgkZl43/VNPee033wRKw6xI8lyjV9FnshARyX3+K+jz6gK7OvUD/qTSUMuR/NbYVfT/wBsh7gXn3MF6y04BrgPWOuceCrRCEQlV3YNR1t5xaaPrddn9ESfv2AzAvqJilnQ99bg+Lxs0uZb6F9qddHV6CxJpgsZOkH0dOAdYaWbzzGyamb1qZmuAe4EFCncRqVO2ITFUxqJup3GwsFmI1YRkyBAOtWjhTa9bR9dd28KtR/JaY130m4FbgVvNrBToBuwFVtddXS8iUid5gJs8O/9ep1kzdvXvzwmLFgFQtmE5z7U7N+SiJF+ldImrc26tc26Wc26Rwl1EGpK3A9zUs+v00+PTwzc0aQBQkbTKo3tYRCQohXv2MGDr+wDUYizs0ejTpCNtpy/g/bcNimSaAl5EjlvbFSsodLUArOp8Mrub5++DVnYNGuTdMgcM2Po+rQ7sDbkiyVcKeBE5bu2XJ45U8/noHaCmTRvvwTNAkavlzE2rQ65I8tVRA97MLjOzt81su5ntMrPdZrYrE8WJSG5otyxxgd3C7gNCrCRL+G6XUze9hCWVI/j/Bb4MdHTOtXPOtXXOtQu4LhHJFc7RbkXiYrJ8P4IHkgNeF9pJSFIJ+PXA0tijXUVEkq1eTbNdXqfe9pbteP+E7iEXlAV8AT90w0o9eEZC0dhY9HVuBaaZ2Qxgf91M59xvAqtKRHLHrFnxybe794tfYJbXeveGrl1h82baHdjjPSf+jDPCrkryTCpH8L8A9gAtgLa+l4hIUsAv7K7ueSDx4Jk6ej68hCCVI/gTnXMXBV6JiOSmt96KT+r8u8/YsfD00970W2/BN74Rbj2Sd1IJ+JfN7CLn3IuBVyMiuWXnTq/7GThkBbzT7bQmvT2bHixzvOq+lofHx8YAGDMmsdDXyyGSKal00d8IvGBme3WbnIgkmTsXYtffruxcyp7iliEXlEWGDmV/3QN33nsPtm4Ntx7JO0cN+NhtcQXOuZa6TU5EkvjPv6t7PllxMUtL+iTac+aEV4vkpZRGsjOzE8xshJmdW/cKujARyQG6wK5RSX/0qJteMuyo5+DN7Hrg20BPYBEwCpgFnBdsaSKSzczVwuzZ8baO4A+X9EePAl4yLJUj+G8DZwEfOOfGAUOBDwOtSkSy3ikfbYAdOwA40L49H3ToFnJF2edtf8DPnQs1NeEVI3knlYDf55zbB2BmzZ1zK4F+wZYlItlu2MbEEKz+J6hJwuZ2ndjYtpPX2LMHli4NtyDJK6kEfJWZdQD+DrxkZs8CG4MtS0Sy3fANK+PTu2JPT5PDqZtewpLKVfSfcc7tcM7dBvwYeBD4dNCFiUh2G+YL+J0DB4ZYSXZ7WxfaSUgaDXgzKzCzeJ+Sc26Gc26Kc+5A8KWJSLZqt6+a0z5a5zUKC9ndT2ftjuTt7r59o4CXDGo04J1ztcA7ZnZShuoRkRwwZOOqROPMM6ltqQFujmRpyalQXOw1Kis5Yc/OcAuSvJHKOfhuwDIze8XMptS9gi5MRLLXsI2J7nlGjw6vkBxwoKgZDBsWbw/1/3EkEqBUxqL/aeBViEhO8Z9/V8CnYNSo+JgBwzau5NVTR4RckOSDowa8c25GJgoRkRxRW5vcRT96NKxbF149OeDGNc25KzY91N/7IRKgo3bR1z1cpt5rvZk9Y2anZKJIEckiy5fT7sAeb7pLF+jdO9x6coB/lL8zN71LQe2hEKuRfJFKF/1v8O57fwww4EqgK7AKeAgoD6o4EclC/ivBx4zRADcp2NSuM5vadKRb9Ue0ObCXfts+CLskyQOpXGQ33jl3r3Nut3Nul3PuPuAS59xfgBMCrk9Eso0/4HX+PWX+2+WSrmEQCUgqAV9rZp+P3RNfYGaf9y1zQRUmIllKAX9M/N30upJeMiGVgL8a+BKwNfb6EnCNmbUEbgqwNhHJNtu3w0rv6PNgQSGUlYVcUO5Y2H1AfFoX2kkmpHIV/Rrgn46weGZ6yxGRrDZnTnxyeZdTOFMD3KRsWdc+HCgoori2hj7bN8BHH0HHjmGXJRGWylX0PWNXzG81sy1m9rSZ9cxEcSKSZXzd83r+e9PsLypmeYnvxiPfH0siQUili/7/gClAd6AH8I/YvKMys/FmtsrMKs1scgPLzczujC1fbGbD6i0vNLO3zey5VD5PRALmC/ikZ51LSvRkOcmkVAK+s3Pu/5xzNbHXw0Dno73JzAqBu4AJwEDgKjOr/8ipCUDf2GsScHe95d8GViAi4Tt0KD4aG8CCHgMaWVkaogfPSCalEvDbzOya2NF0oZldA3yUwvtGAJXOuTWxp889AUyst85E4FHnmQ10MLNu4J0aAC4FHkj5qxGR4CxfDtXVAGxpcyIb2h3173ypZ6H/j6I5c7w/mkQCkkrAfxX4PLAZ2ARcAXwlhff1ANb72lWxeamu87/ArUBtCp8lIkHzn3/v3l8D3ByDDe06s6XNiV6jutr7o0kkIKmMZNfLOXe5f4aZjQWONvh0Qz/99e+bb3AdM7sM2OqcW2Bm5Y1+iNkkvO59SkpKqKioOEpZqauurk7r9nKd9kdCVPfFLYNrAJK+trp5m/70DN1i89oO68stg2vi61VXV3PL4PCPRhuqO8jPaegz/N8bDS3fPeA0SuZ5pzpWPfwwm/7pSDcpRUNUf1aOVSb3RyoB/ztgWArz6qsCevnaPfGGvE1lnSuAy83sEqAF0M7M/uScu6b+h8RG1rsPoKyszJWXlx+lrNRVVFSQzu3lOu2PhKjui+smTwVg7dXlh827ee3a+Lz/LRzI/CVF8fUqKir49cxPMlXmkS3x15DKr7djU/vq8pcAACAASURBVPd11+0bv4fHt45/bzS0fHfrAfwbXsD3276dfmn8Piqt+/+749K0bfN4RfVn5Vhlcn8c8SfAzEYDY4DOZvZd36J2QGEK254H9DWz3sAGvDHsv1hvnSnATWb2BDAS2Omc2wT8MPYidgT/vYbCXUQyo/3e3fEBbigqYknJqeEWlMOSbi/UhXYSoMb+xC0G2sTWaeubvwvvCLtRzrkaM7sJmI73B8FDzrllZnZDbPk9wDTgEqAS2ENq5/ZFJMOGbFrtawxhf7Pm4RWT45aUnMrBgkKa1R6CVau80QFPPDHssiSCjhjwsefAzzCzh51zHwCYWQHQxjm3K5WNO+em4YW4f949vmkH3HiUbVQAFal8nogEI+nhKKNGhVdIBOxv1pxlJacwZNO73ow5c2DChHCLkkhK5Sr6X5pZOzNrDSwHVpnZ9wOuS0SySNLY6XrAzHF7WwPeSAakEvADY0fsn8Y7Gj8J74EzIpIHzNUmd9HrCP64KeAlE1IJ+GZm1gwv4J91zh1Ej4kVyRt9Pqqi3f7Y1eldukDv3uEWFAFJF9ppwBsJSCoBfy+wFmgNvG5mJ+NdaCcieSDp/Pvo0RrgJg2q2nWBrl29xu7dGvBGAnHUgHfO3emc6+GcuyQ2pOwHwLgM1CYiWSDp/Lu659PDLPlaBnXTSwAauw/+Gufcn+rdA+/3m4BqEpEsMkwX2AVj9Gh45hlvetYsmDQp3Hokchq7D7517N+2jawjIlG2cyd9t8UeF1FYCGVl4dYTJTqCl4A1dh/8vbF/f5q5ckQkq8ydS0HdNbVnnAGtWze+vqRu+HAoKoKaGg14I4ForIv+zsbe6Jz7VvrLEZGs4j+y1Pn39GrZEoYOhXnzvPbs2XDJJeHWJJHSWBf9At/0T4GfBFyLiGSb2bMT0zr/HlfawENk6izZsLPBh8w0aPToRMDPmnVMAd9YLZLfGuuif6Ru2sy+42+LSB6orU0OeB3Bp9/o0XBnrLNU5+ElzVK5Dx40sI1I/nn3Xfj4YwC2t2wHp+oJcmnn7xXRgDeSZqkGvIjkG98R5dvd+2mAmyCcdBJ06+ZNV1drwBtJqyMGvJntNrNdZrYLOKNuum5+BmsUkTD4uucX+sdOl/TRgDcSoCMGvHOurXOuXexV5Jtu65xrl8kiRSQE9Y/gJRj+axsU8JJG6qIXkcPt3g1LlwJQi/FOt9NCLijCdAQvAVHAi8jh5s3zrqIHVnU+mU+atwq5oAirG/AGEgPeiKSBAl5EDpfUPa/z74GqG/Cmjv/WRJHjoIAXkcP5L7DroYAPnLrpJQAKeBFJ5lxSwOsCuwxQwEsAFPAikuTkHZtg2zavccIJrDmxR7gF5QMNeCMBUMCLSJJhG3zPfx85Emf6NRG4+gPeLFsWbj0SCY09bEZE8kD9h5UM3bgq0Rg9GvZkuKA8VPrDadzdtjcTNm3yZsya5T2eV+Q46E9zEUkybKPvCF4PmMmYpNECdR5e0kABLyJxLQ/so//W972GGYwcGW5BeSTpbgUFvKSBAl5E4s7Y/C5FzhvghgEDoH37cAvKI0u7ngrNmnmN1avho4/CLUhyngJeROIOO/8uGbO/qDh5wJs5c8IrRiJBAS8iccM3rEg0dP4983Q/vKSRAl5EPM4xzB/wY8aEV0u+0pPlJI0U8CICQO+PN9Jx7y6vccIJ0F9D1GacBryRNFLAiwhQr3t+9Ggo0K+HjNOAN5JG+gkWEQCGVy1PNMaODa+QfGam8/CSNgp4EQFguH+IWgV8eBTwkiYKeBGh/d7dnPbROgAOFhTCWWeFXFEeU8BLmmgsehFhqG942mUlpzCkVavD1qk/Zr1/3i2Da9CvkzQZPtwb8ObgwcSANx07hl2V5CAdwYsIZb4L7BZ2HxBiJUKLFskD3syeHV4tktMU8CKSdAX9/J4DQ6xEAHXTS1oo4EXyXNGhGoZsXB1vL+ih+99Dp4CXNFDAi+S5gVvX0LJmPwBV7bqwpW2nkCuSpFEE58zxzseLNJECXiTP+bvnF/TQ+fes0KuXN+gNwCefwDvvhFuP5KRAA97MxpvZKjOrNLPJDSw3M7sztnyxmQ2LzW9hZnPN7B0zW2ZmPw2yTpF8NrzKf/5dAZ81zj47MT1zZnh1SM4KLODNrBC4C5gADASuMrP6V+9MAPrGXpOAu2Pz9wPnOefOBIYA481Mj7YSSTfnKNuQGMFuoY7gs4cCXo5TkEfwI4BK59wa59wB4AlgYr11JgKPOs9soIOZdYu1q2PrNIu9XIC1iuSndevoWr0dgOrilqzsXBpuPZJQP+CdfgVK0wQZ8D2A9b52VWxeSuuYWaGZLQK2Ai855+YEWKtIfnrrrfjk2936caigMMRiJMmgQdC+vTe9ZQu891649UjOCXLoKWtgXv0/QY+4jnPuEDDEzDoAz5jZ6c65pYd9iNkkvO59SkpKqKioOK6i/aqrq9O6vVyn/ZEQlX3R98kn4391Nx/WLzYiHUlfW928xpS0TG29fJDqvmhoH9f/nhrcvz8d53jHNisfeIDN48cftp2GPiubvjej8rOSLpncH0EGfBXQy9fuCWxs6jrOuR1mVgGMBw4LeOfcfcB9AGVlZa68vPx4646rqKggndvLddofCZHZF9/9bnzyd80G8cYS71fC2qvL4/Ova2CI2vpuGVzDr5doqFpIfV80tI/98wCYONG7TQ7o/9FH9G/ge66h/5/DthOiyPyspEkm90eQXfTzgL5m1tvMioErgSn11pkCXBu7mn4UsNM5t8nMOseO3DGzlsAFwEpEJH12747fflWLsah7v5ALksPoQjs5DoH9ye2cqzGzm4DpQCHwkHNumZndEFt+DzANuASoBPYAX4m9vRvwSOxK/ALgSefcc0HVKpKX5s6F2loAVnU+md3NW8cXNfRgGUmvo+3j0slTaV5zgFXFxXDgAKxcybBvPcb2Vt55+bV3XJqJMiWHBdqn5pybhhfi/nn3+KYdcGMD71sMDK0/X0TSyHdEqAFustP+omIoK4tfDDl8wwpe6qs7hiU1GslOJF+9/np8cm6vQSEWIo3yddOXVS1vZEWRZAp4kXx04EDSQ0zm9VTAZ62xY+OTZ1UtC7EQyTUKeJF8tHAh7N0LwPr2JWxq1znkguSIfA+eOX3ze7Q4uC/EYiSXKOBF8tEbb8Qn5+r579mtUycY4F0jUVxbw5mb3g25IMkVCniRfOQP+F6nh1iIpETn4eUYKOBF8k1tbdIV9Dr/ngN8AX+WAl5SpIAXyTfLlsHHH3vTXbqw5sT6j4iQrOML+GEbVlBQeyjEYiRXKOBF8o3v9jjOOQesoUdCSFbp3Ru6dQOg3YE9nLZtXcgFSS5QwIvkG9/5d849N7w6JHVm9brpdbucHJ0CXiSfOHf4EbzkBl/Aj1x32HO3RA6jgBfJJ2vWwKZN3nS7dnDGGeHWI6nzPYFs5Pql3h9rIo1QwIvkke/f/LtEY+xYKCwMrxhpmtNP5+MWbQHovGcHrEztAZulk6fq4UF5SgEvkkeSzt3q/HtuKShIfmbAjBnh1SI5QQEvkkdGrPcFvM6/55w5vQYnGhUVodUhuUEBL5IvNm6kdId3/n1/YTPvMaSSU+ac5Bt1cMYMnYeXRingRfKF7/a4t7v3g+bNQyxGjsWKzqXsbN7aa2zeDKtXh1uQZDUFvEi+8J2znavhaXNSbUGhzsNLyhTwIvnitdfik7NO1u1xuWq2/zy8Al4aoYAXyQcbN8Zvq9pfWMTC7v1DLkiO1ZyT6l1op/PwcgQKeJF84Lvi+u3u/dnfTOffc9XyLr3ZVdzKa2zcyMmxCydF6lPAi+QDf/f8Seqez2W1BYXM852HH7VuSYjVSDZTwIvkA51/j5Q5vRK3y41cr3HppWEKeJGoW78e3nsPgH1FxSzq1i/kguR4zfadhx+1bonOw0uDFPAiUec7ep/fYwAHipqFWIykw7KSPtDWG5e+++5t9Nq5JeSKJBsp4EWizhfwb518ZoiFSLocKiikotNp8faodYtDrEaylQJeJOp8Ae/v2pXc5v+/HPOBAl4Op4AXibL334cPPvCmW7dmcde+4dYjafOmrzdm7Afv6Dy8HEYBLxJlvqN3zjmHmsKi8GqRtFpWcgo7WrQBoMsnH9N327qQK5Jso4AXiTJ/wI8bF14dkna1BYW85RvT4OwPFoVYjWQjBbxIVDkHr76aaCvgI+et0iHx6bFrFfCSTAEvElWrV3tj0AO0awdDh4Zbj6TdTN95+JHrl1J0qCbEaiTbKOBFouqllxLT5eVQpPPvUbP2hO5UtesMQNsDezlj07shVyTZRAEvElX+gL/oovDqkOCYJY1toPPw4qeAF4migweTL7C78MLwapFAzfSfh//gnRArkWyjgBeJorlzYfdub/qkk6Cv7n+PKv8R/NANK2l1YG+I1Ug2UcCLRNGLLyamL7oIzMKrRQK1rfUJrOx0MgDFtTWMWL8s5IokWyjgRaLIf/5d3fOR96avm36MuuklRpfViuSA0slT49Nr77i08eWTx3pd9OAduZ9/fuD1SXr4/x8bm1ffzNIhfG3+s4AutJMEHcGLRM1rr8GhQ970sGHQsWO49Ujg5vYcxMGCQgAGbn0ftm4NuSLJBgp4kajR7XF555PmrVjYvX9ihv8aDMlbgQa8mY03s1VmVmlmkxtYbmZ2Z2z5YjMbFpvfy8xeM7MVZrbMzL4dZJ0ikaLz73lpxinDE43nnw+vEMkagQW8mRUCdwETgIHAVWY2sN5qE4C+sdck4O7Y/BrgFufcAGAUcGMD7xWRenru2AyVlV6jVSsYMybcgiRjZvQelmhMn544TSN5K8gj+BFApXNujXPuAPAEMLHeOhOBR51nNtDBzLo55zY55xYCOOd2AyuAHgHWKhIJ5/gfOPKpT0Hz5uEVIxm1vOQUPmzdwWt89BEsXBhuQRK6IK+i7wGs97WrgJEprNMD2FQ3w8xKgaHAnIY+xMwm4R39U1JSQkVFxfFV7VNdXZ3W7eU67Y+ETO+LWwYnHiLS0OfWLb/k5XnxeZW9e1NVb92GtuOfd6xKWqZnO1EQ5r7YNnQonWd6Ixi+/4c/8MGXvxyvJayfXf3eSJbJ/RFkwDc0soZryjpm1gZ4GviOc25XQx/inLsPuA+grKzMlZeXH1OxDamoqCCd28t12h8Jmd4X1/lvg7v68M+9bvJUimsO8pUli+PzTv3Wtzi1X7+jbue6FG7DOppbBtfw6yW66xbC3RerTyjjd3gB33vVKnqXl8f/fxv6vskE/d5Ilsn9EWQXfRXQy9fuCWxMdR0za4YX7n92zv0twDpFIqGsahlt6oYp7dMHTjst3IIk497oPZRDFvu1PmcObN8ebkESqiADfh7Q18x6m1kxcCUwpd46U4BrY1fTjwJ2Ouc2mZkBDwIrnHO/CbBGkcgYt2Z+onHJJRqeNg/taNmOd7rFnjtQW5t8R4XkncAC3jlXA9wETMe7SO5J59wyM7vBzG6IrTYNWANUAvcD34zNHwt8CTjPzBbFXpcEVatIFJz3Xr2Al7w0o7dulxNPoCeKnHPT8ELcP+8e37QDbmzgfTNp+Py8iDTgpI830Wd7lddo2dK7gl7y0oxThvOvbz7mNV54AfvyP+NMY5rlI/2vi0RAub97/rzzvJCXvLS466mJ4Ym3bPGGrpW8pMteRSLgsPPvNO0BNRIdtQWF3hDFjz8OQPl781lW0ueo3w9HU/f+Y3mvhENH8CK5bs8eRq9bkmjr/LtcmgjhCyrnhliIhEkBL5LrXnuNFjUHAFjd8SQoLQ23HgnfhAlQ6D1dbuimVXSu/jjkgiQMCniRXDc10fX6Wp+yEAuRrHHiiXDOOfHmee/pKD4fKeBFcplzMCUxvETFKQp4ibn88vjkBZUNjvQtEaeAF8llCxbAhg0A7GjRhrm9BoVckGQNX8Cfs3YRLQ7uC7EYCYMCXiSX/f3v8clXTh3BoYLCEIuRrNKnDwz0nrLdouYAZ699J+SCJNMU8CK57Nln45MvnjoqxEIkK6mbPq8p4EVyVWUlLF0KwL6iYl7vPSzkgiTr+AL+/Mq5mKsNsRjJNAW8SK7yHb2/UTqEvcUtQixGstKIEXzYqgMAnffsYMjG1SEXJJmkgBfJVb7z7y+pe14aUljIq33OijcvrJwdYjGSaQp4kVy0dSu89ZY3bcYrp44Itx7JWi/1TfzxN37VW96tlZIXFPAiuei557znfQOMHctHrTuEW49krTdKh1Bd7D186JSPN9L/w7XhFiQZo4fNiISooQe+pPQwj7/9LT7582anHdPnSHQl/X83a86rfc7i8hWvAzBh1ZvATeEUJhmlI3iRXLNjB7z4Yrw5/bQxIRYjuWBqv7Pj05eunKlu+jyhgBfJNc8+CwcPAvBO176s79A15IIk2804ZRh7mjUH4NTtVbB8ecgVSSYo4EVyzZNPxien9j+7kRVFPPuateDVPr4LMZ96KrxiJGMU8CI5pP3e3Und89MU8JKiaf3GJhp//Wt4hUjGKOBFcshF786CmhoAFnU7jar2JSFXJLnitVPK2FvkddOzbBmsWBFuQRI4BbxIDrls5cz49HM6epcm2FvcgtdOGZ6YoW76yFPAi+SIDnt3MXbtonj7+X4KeGma5/3d9I8/rqvpI04BL5IjLl49i6K6h4WMGsWG9l3CLUhyzsunjuSTZrFnFqxYAYsWNf4GyWkKeJEc8enlFYnG5z8fWh2Su/YWt2D6aaMTM/785/CKkcAp4EVyQI+dWxm9bonXKCiAK68MtyDJWc8OLE80HnsMDh0KrRYJlgJeJAdM9B+9X3ABdOsWWi2S22aWDoEusdM7mzbBa6+FW5AERgEvku2c47NLX020r702vFok5x0qKEzuAVI3fWTpYTMi2W7BAm94UeCTZi0oW9CCvUuO/PAYPVhGjubyHaVMiU3vfuwvlJ34T+yPDWWb0sOOYuq+15ryHskcHcGLZLs//jE++UK/MewtbhFiMRIFi7v25b0TewDQ9sBeLqicG3JFEgQFvEg2O3jQu1855m+DzguxGIkMs6SL7T635OXwapHAKOBFstn06fDhhwBsbnMis04aHHJBEhV/GzQuPn3u+wvptuvDEKuRICjgRbLZ/ffHJ/8+aBy1BYUhFiNRUtWhK2+cPASAAhxX6Cg+chTwItlq40aYmrhg7okzLgqxGImiv5yZ+J76wuKXsLqREiUSFPAi2er//i8+CMmskwazNnZRlEi6vNh3NB+3aAtAz11bGbv2nZArknRSwItko9paeOCBePPxM8eHWIxE1YGiZjzjOxd/5eIXQ6xG0k0BL5KNXn4Z1q71pk88MXn8cJE08nfTX/juLNi2LcRqJJ0U8CLZyHdxHddey/6i4vBqkUhb1bmUt7v1A6D5oRp45JGQK5J0UcCLZJtNm+DZZxPtr389vFokLzx+5sWJxl136QE0EaGAF8k299zjDXADcPbZMHBguPVI5E0ZeC47WrTxGu+/D9OmhVuQpEWgAW9m481slZlVmtnkBpabmd0ZW77YzIb5lj1kZlvNbGmQNYpkk+Kag17A1/nWt8IrRvLGvmYtkm/D/N3vwitG0iawgDezQuAuYAIwELjKzOofikwA+sZek4C7fcseBnTpsOSVy1a+Dlu3eo2ePekzr7keHiMZ8adhl3LIYpHw0kuwcmW4BclxC/IIfgRQ6Zxb45w7ADwBTKy3zkTgUeeZDXQws24AzrnXge0B1ieSXZzjugX/SLRvvNF7tKdIBlS1L+GVU0ckZvz+9+EVI2kRZMD3ANb72lWxeU1dRyQvDNuwkjM2V3qNFi3g+uvDLUjyzsPDLks0HnkEdu4Mrxg5bkE+D94amOeOYZ3GP8RsEl73PiUlJVRUVDTl7Y2qrq5O6/ZynfZHQrr2xS2Da+LT4197Jj696bzzWLV0adLybFbSkpypNWg5vS9OH8Qns0+m9QcfQHU17916K+uvugpIfE3+7/uG5tWn3xvJMrk/zLkm5WnqGzYbDdzmnLs41v4hgHPul7517gUqnHOPx9qrgHLn3KZYuxR4zjl3eiqfWVZW5ubPn5+2r6GiooLy8vK0bS/XaX8kpGtf1J1f7719A6/cfwMFdX/fvvMOnHFGzpx/v2VwDb9eEuTxQu7I9X2x9rQt8LWveY2uXb2r6lu0iH8vrr3j0vi6Dc2rT783kqV7f5jZAudcWUPLguyinwf0NbPeZlYMXAlMqbfOFODa2NX0o4CddeEukk/+Zc7TiXAfPx7OOCPcgiR/XX01dO/uTW/eDI8+Gm49cswCC3jnXA1wEzAdWAE86ZxbZmY3mNkNsdWmAWuASuB+4Jt17zezx4FZQD8zqzKzrwVVq0iYuu7axmeXvpqY8cMfhleMSPPm8N3vJtr/7/9p4JscFWg/knNuGl6I++fd45t2wI1HeO9VQdYmki2un/cMxbXeucz5PQZQds45IVckeW/SJPj5z2HHDqishL/9DWgVdlXSRBrJTiREHfbu4qp3psfbfxj1ObCGrj0VyaC2beFG37HXL38JAV2vJcFRwIuE6Otzn6H1wX0ArOx0Mq/2OSvkikRivvUtaNnSm377bS5+d1a49UiTKeBFwrJlC19ZkLju9PdjvqCjd8keXbrAN+OXRfGvb/wZc7UhFiRNpYAXCcsdd9Dq4H4AVnQuZWr/s0MuSKSeH/wAWrcGoP+2D7hsxRshFyRNoYAXCUNVFdydePTCr8/5Es704yhZpnNn+Pa3483vvPk41OToID55SL9RRAJWOnnq4QPW/PznsN87el/U7TRe9o8BLhKiuu/X+Pfs974H7dsD0Gd7FfzxjyFWJ02hgBfJtGXL4IEH4s3/PudLOvcu2euEE5Lvi//Rj6C6Orx6JGUKeJFMcs77ZRkbOOTNk89gZumQkIsSOYrvfpctbU70pjdtgl/9Ktx6JCUKeJFMev55ePFFb7qggJ+d93UdvUv2a9PG62mq89//DevXH3l9yQoKeJFMOXgwuavz+utZ2aV3ePWINMFTg89naUkfr7F3r4ZUzgEKeJFM+e1vYdUqb7ptW/jZz8KtR6QJnBXws/OuT8z4858ZuW5JeAXJUSngRTKg584t8JOfJGb8+MfeQCIiOWTOSYPhn/853r59+l0U1xwMsSJpjAJeJGjO8Z8v3g179njt00+H73wn3JpEjtVvf+v1QOHdNnfDnKdCLkiORAEvErBLV87kvDXzvYYZ3H8/NGsWblEix6pHD7j99njzxll/SZx6kqyigBcJ0tat3PbyvYn2N74Bo0aFV49IOnzjGzDCG5yp+aEa+OpXNcJdFlLAiwTFOfj61+m8Z4fX7t496chHJGcVFsJ993GwoNBrv/UW/Nd/hVuTHEYBLxKUBx+EKYmnxfHQQ/EhP0Vy3pln8tuxVyXat90G8+eHVo4cTgEvEoR33026kO7hYZfBxReHWJBI+t096nMs6N7fa9TUwDXXwCefhFuUxBWFXYBIrqv/IJmWB/ax4uWfxn/RVZ7YkzvKr+M233pr77g05e2JZKtDBYX862W3MO3hb9HmwF5YtYq/j7iU71z2vfgIjbcMrqE83DLzlo7gRdLJOX7x4l2wJDYASHEx3/6n77GvWYtw6xIJyLoTunHbBf8Sb396+QyuXfhciBVJHQW8SBpd8/Y0PrvstcSMu+5iWddTwytIJAOeGnwBj59xUbz941cfYFjVihArElDAi6TNOe8vTL4l7qtfheuvP/IbRCLktgtvYHHsj9lmtYe495lf0HPH5pCrym8KeJE0GLB1DX/4+y8pcrXejGHD4Pe/D7cokQzaX1TMNz79b2xv2Q6Aznt28Mhfb6N59e6QK8tfCniR49Rt14c89Nef0vbAXgA2tO0M//gHtGwZcmUimbWhfRcmffZH7C/0Rmrss72Ky35zu/f0Ock4BbzI8diwgccf/ze6VX8EwK7iVnz1cz/xBrURyUPzew7iXy+7Jd7uvnoFfOYzsG9fiFXlJwW8yLHavBnOO4/SHZsAOFBQxDc+82+s6lwabl0iIZvW/2x+Pu6riRnTp3tPodu/P7yi8pACXuQYNN+8GcrLYfVqAA4WFPLNT/+QN0uHhFuYSJZ4YMRn+Z+xX0zMmDbNO5LXQDgZo4AXaarFixl2003xJ2jVWAE3X34rL/cdGXJhItnlt2OvYu7EzyVmPP88nHcefPhheEXlEQW8SFO89BKcey7NP/LOuVNczE0Tf8AL/caGW5dINjJjzhVfhB//ODFv7lwYM8YbzlkCpYAXSYVz3tOyxo+HnTu9ee3awQsvKNxFGmMG//mf3m2jseFrqayEsjLvbhMJjAJe5Gi2b4crroDJk6HWu899f8eO8PrrMG5cyMWJ5Igbb4Snn4bmzb32rl1w+eXw7/8OBw+GW1tE6WEzkhdKj/FBL59as4BfPf9bSqq3J2aefTZ/uvJr/OzxKni8KqXtNHW5SCR95jPw5pveFfUffODN+8Uv4IUX4JFHYNCgw95ytJ+Vhn6e697T2M96PtARvEgDTtizk18+fyeP/PUnyeF+003wyivs6XBCeMWJ5LLhw2HBAl4vHZqYt2CBN/rj7bfrVro0UsCL+BTWHuKahVN57f5/4arFL8bnf9iqA9d/9sfwu99BcXGIFYpEQMeOXPe52/hF+VcTP08HDsCPfgSnnw5T1cOVDgp4EYBDh+DPf+bFB7/Jz1+6mw77quOLXjhtNBd/7S7dBieSRrUFhdw/8rOwcKF39F6nshIuu8y7nW7GjPAKjAAFvOS3vXvhwQdh8GC45hr6bN8QX7S2Qze++s//wQ2f+RHbW7UPsUiRCBs0CObMgTvvhA4dEvNfew3Ky3n88R9S/t48rO5BTpIyXWQn+Wn1ai/YH3jAu0reZ1fz1twz8p958KxPs79I3fEigSsqgptvhiuv9O6Zf+ABr1cNGL1uCaPXLeGDDl3505BLeHrw+fqD4aehlgAACPlJREFUO0UKeMkb3XZ9yCWr3oSzboP58w9foW1bfjv4Mh48ayK7WrTJeH0iea9zZ7jnHrj1Vu/q+kceiQf9yTs286OKh/jBjId56+QzmTLwU7zYd1TIBWc3BbxE1549MGsWvPAC0x9+kn7b1jW8Xmmpd4/u177G//zXWxktUUQacMopXg/bv/87933hFr6w+EXa7/fGsC9ytZy79m3OXfs2NVYA838HF1/svYYN83oDBFDAS1QcOOB1u7/zjhfqs2d70zU1APSrv35xMVx6KXzlK3DJJVBYmPGSReQoevfm9vO+xm/OuZrLl7/OFUtfZkTV8vjiIlfr3Vf/5pvwH/8BrVrBWWdx6/7OLOrWD9YM8P6AL8jPy80CDXgzGw/8FigEHnDO3VFvucWWXwLsAa5zzi1M5b2Shz75BNat8wbIqPt31SpYtswb1zrWlXck+wuLmNNrMOf++Cb47GeTL+gRkay1r1kLnjzzIp488yK679rKpStmcumqNzhjUyUFuMSKe/bAjBl8s679zC+gZUsYMMC7mO/UU+Hkk73XSSdBz56Rvu01sIA3s0LgLuBCoAqYZ2ZTnHPLfatNAPrGXiOBu4GRKb5XcoFzXvAePOi99u71grq62vvXP11d7Q1f+dFHsG1b8mvrVvj446Z//oABcP75fGVzR2b3Gsze4has/Wp+j24lkss2tuvC/SM/y/0jP8uJe3aycBTeSHivvgobNhz+hr17vVvxFi48fJkZdOrknfv3vzp18g4A2rSBtm29l3+6VSvvD4Pmzb1/i4uzshcwyCP4EUClc24NgJk9AUwE/CE9EXjUOeeA2WbWwcy6AaUpvDc4v/41vPYag7dtg44dvXnOJf97pOlMzAvp84ZXV0Pr1ol5dcFdU5MI8IZemVJa6v2VXlYGo0fDyJHxo/TXNDSsSORsb9UevngpfDH23PmqKpgzh3t/9RiDN1cyZt9m7+DgSJzzHl2bjsfXFhYmwr4u+Js18+YXFnrXBhQWMnzvXpgyBfr3P/7PPIogA74HsN7XrsI7Sj/aOj1SfG9wFi2CqVPpmLEPzA1twy6gWTPo1SvRvXbyyd7FOIMGeT8sbXTlu0he69kTevbkl/NaALGx6Ldtg+XLvdfatd6pvbrXpk3JBzLH49Ahr7dg795GV2sLsG9fej7zKMyl64urv2GzzwEXO+euj7W/BIxwzt3sW2cq8Evn3MxY+xXgVuCUo73Xt41JwKRYsx+wKo1fRidgWxq3l+u0PxK0L5JpfyRoXyTT/kiW7v1xsnOuc0MLgjyCrwJ6+do9gY0prlOcwnsBcM7dB9x3vMU2xMzmO+fKgth2LtL+SNC+SKb9kaB9kUz7I1km90eQ9w7MA/qaWW8zKwauBKbUW2cKcK15RgE7nXObUnyviIiIHEFgR/DOuRozuwmYjner20POuWVmdkNs+T3ANLxb5CrxbpP7SmPvDapWERGRqPn/7d1biFVVHMfx7w8RFQ3KsDAy8sGHTMygJKKHMMEpQisQiiChJ0GhoIc0oegiPQgRBNFLkQ+aCHaRStCsMAizFzNjrITohigUUhEZo78e9hrnzHRmHJTZR/f+feAwe699zpl1fnP5s29rTeh98LY/pCrinW2vdSwbWDPe1/bAhBz6v4QljyHJYrjkMSRZDJc8hqstjwm7yC4iIiJ6p53j90VERDRcCvwIkp6XdEjSQUm7JV3TsW29pKOSvpW0rJf9rIukTZKOlEzekXR5x7Y25rFS0jeSzki6ZcS2NubRVz7vUUnret2fukl6Q9IJSYc72mZK2iPp+/L1il72sS6S5kj6RFJ/+Rt5rLS3NY+pkg5I+qrk8Wxpry2PFPj/22R7oe1FwPvA0wCS5lNdzX8j0Ae8WobUbbo9wALbC4HvgPXQ6jwOAw8A+zob25hHx5DSdwPzgYdKDm3yJtXPu9M6YK/tecDest4GA8ATtm8AbgPWlN+HtuZxClhi+yZgEdBX7harLY8U+BFs/9GxOh3OzmSwAthm+5TtH6iu/F9cd//qZnu37YGyup9qTAJobx79trsNptTGPM4OR237X2BwSOnWsL0P+H1E8wpgc1neDNxXa6d6xPaxwcnCbP8J9FONStrWPGz7r7I6uTxMjXmkwHchaaOkn4GHKXvwjD6sbps8Cuwqy8ljuDbm0cbPPB5Xl/E8KF+v6nF/aifpeuBm4AtanIekSZIOAieAPbZrzaOVBV7SR5IOd3msALC9wfYcYAuwdvBlXd6qEbcgnCuP8pwNVIfgtgw2dXmr1uTR7WVd2hqRxxja+JnjHCTNAHYAj484Ito6tk+X073XAoslLajz+0/offAXK9tLx/nUrcAHwDOMb+jdS9K58pC0CrgXuMtD91W2No9RNDaPMbTxM4/HcUmzbR8rs2OOMZ1Zs0iaTFXct9h+uzS3No9Btk9K+pTqeo3a8mjlHvxYJM3rWF0OHCnLO4EHJU2RNJdqDvsDdfevbpL6gCeB5bb/7tjUyjzG0MY8MqR0dzuBVWV5FfBeD/tSG0kCXgf6bb/UsamtecwavOtI0jRgKVU9qS2PDHQzgqQdVLPSnQF+BFbb/rVs20B1HnqA6vDTrlHfqCEkHQWmAL+Vpv22V5dtbczjfuAVYBZwEjhoe1nZ1sY87gFeZmhI6Y097lKtJL0F3Ek1Q9hxqqN97wLbgeuAn4CVtkdeiNc4ku4APgO+pvr/CfAU1Xn4NuaxkOoiuklUO9PbbT8n6UpqyiMFPiIiooFyiD4iIqKBUuAjIiIaKAU+IiKigVLgIyIiGigFPiIiooFS4CMiIhooBT4iIqKBUuAj4rxIulXSoTLv9fQy53WtY21HxOgy0E1EnDdJLwBTgWnAL7Zf7HGXIqJIgY+I81bGoP8S+Ae43fbpHncpIoocoo+ICzETmAFcRrUnHxEXiezBR8R5k7QT2AbMBWbbXtvjLkVE0cr54CPiwkl6BBiwvVXSJOBzSUtsf9zrvkVE9uAjIiIaKefgIyIiGigFPiIiooFS4CMiIhooBT4iIqKBUuAjIiIaKAU+IiKigVLgIyIiGigFPiIiooH+A5rRhv7+ukA8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see how the MLE performs\n",
    "mu_real = 5\n",
    "var_real = 36\n",
    "num_samples= 1000\n",
    "samples = np.random.normal(mu_real, np.sqrt(var_real), size=(num_samples))\n",
    "\n",
    "mu_mle = np.sum(samples) / num_samples\n",
    "var_mle = np.sum(np.square(samples - mu_mle)) / num_samples\n",
    "\n",
    "print(\"mu mle: \", mu_mle)\n",
    "print(\"var mle: \", var_mle)\n",
    "\n",
    "x = np.linspace(-30, 30, 10000)\n",
    "f_x_mle = (1 / np.sqrt(2 * np.pi * var_mle)) * np.exp(-0.5 * (np.square(x - mu_mle)) / var_mle)\n",
    "\n",
    "# set bins for histogram\n",
    "n_bins = 100\n",
    "bins_edges = np.linspace(samples.min(), samples.max() + 1e-9, n_bins + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid()\n",
    "ax.set_ylabel('Histogram (PDF)')\n",
    "ax.set_xlabel('x')\n",
    "\n",
    "# plot histogram\n",
    "ax.hist(samples, bins=bins_edges, density=True)\n",
    "# plot estimation\n",
    "ax.plot(x, f_x_mle, linewidth=3, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - MLE for m-Dimensional Gaussian\n",
    "Given $\\{x_i\\}_{i=1}^n$ i.i.d samples of $X \\sim N(\\mu, \\Sigma)$, what is the MLE?\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "The final results are pretty much the same, but with vectors and matrices, though the math is a little more complicated.\n",
    "$$ \\hat{\\overline{\\mu}}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n \\overline{x_i} $$\n",
    "$$ \\hat{\\Sigma}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n (\\overline{x_i} - \\hat{\\overline{\\mu}}_{MLE}) (\\overline{x_i} - \\hat{\\overline{\\mu}}_{MLE})^{T}$$\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/scissors.png\" style=\"height:30px;display:inline\"> Vector & Matrix Deriviatives\n",
    "* $\\nabla_x Ax = A^{T}$\n",
    "* $\\nabla_x x^{T} A x = (A + A^{T}) x$ \n",
    "* $\\frac{\\partial}{\\partial A} \\ln |A| = A^{-T}$\n",
    "* $\\frac{\\partial}{\\partial A} Tr[AB] = B^{T}$\n",
    "\n",
    "Using the above, we will use the following:\n",
    "1. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} x_i = \\Sigma^{-1} x_i$ \n",
    "2. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} \\mu = (\\Sigma^{-1} + {\\Sigma}^{-T}) \\mu$\n",
    "3. $\\frac{\\partial}{\\partial \\Sigma^{-1}} \\ln |\\Sigma^{-1}| = \\Sigma^{T} = \\Sigma$\n",
    "4. $\\frac{\\partial}{\\partial \\Sigma^{-1}} Tr[\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}] = \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}$\n",
    "\n",
    "#### Solve for the d-dimensional case\n",
    "* $p(x|\\mu, \\Sigma) = \\frac{1}{(2\\pi)^{\\frac{nd}{2}} |\\Sigma|^{\\frac{n}{2}}} e^{- \\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^{T} \\Sigma^{-1} (x_i - \\mu)}$\n",
    "* $\\ln p(x|\\mu, \\Sigma) \\propto -\\frac{n}{2} \\ln |\\Sigma^{-1}| -\\frac{1}{2} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu}) $\n",
    "* $\\nabla_{\\mu} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu}) = \\sum_{i=1}^{n} (-2\\Sigma^{-1} \\overline{x_i} + (\\Sigma^{-1} + {\\Sigma}^{-T}) \\mu) = 0 $ $$ \\rightarrow \\hat{\\overline{\\mu}}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n \\overline{x_i} $$\n",
    "\n",
    "* **The Trace Trick** - $\\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu}) = \\sum_{i=1}^n \\textit{Trace}\\big((\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu})\\big) = \\textit{Trace}\\big(\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T} \\big)$\n",
    "* $\\frac{\\partial}{\\partial \\Sigma^{-1}}\\big( \\frac{n}{2} \\ln |\\Sigma^{-1}| -\\frac{1}{2} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu}) \\big) = \\frac{\\partial}{\\partial \\Sigma^{-1}}\\big( \\frac{n}{2} \\ln |\\Sigma^{-1}| -\\frac{1}{2} Tr[\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}] \\big) = $ $$ \\frac{n}{2} \\Sigma - \\frac{1}{2} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T} = 0 $$ $$ \\rightarrow \\hat{\\Sigma}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n (\\overline{x_i} - \\hat{\\overline{\\mu}}_{MLE}) (\\overline{x_i} - \\hat{\\overline{\\mu}}_{MLE})^{T} $$\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/infinity.png\" style=\"height:50px;display:inline\"> Asymptotic Properties of MLEs\n",
    "Let $X_1, X_2, ..., X_n$ be a random sample from a distribution with a parameter $\\theta$. Let $\\hat{\\theta}_{ML}$ denote the MLE of $\\theta$. Then (under some conditions):\n",
    "\n",
    "1.  $\\hat{\\theta}_{ML}$ is **asymptotaically consistent**: $$ \\lim_{n \\to \\infty} P(|\\hat{\\theta}_{ML} - \\theta| \\geq \\epsilon) = 0, \\forall \\epsilon >0 $$\n",
    "2. $\\hat{\\theta}_{ML}$ is **asymptotaically unbiased**: $$ \\lim_{n \\to \\infty} \\mathbb{E}[\\hat{\\theta}_{ML}] = \\theta $$\n",
    "3. As $n$ becomes large, $\\hat{\\theta}_{ML}$ is approximately a **normal random variable**, that is, the random variable: $$ \\frac{\\hat{\\theta}_{ML} - \\theta}{\\sqrt{Var(\\hat{\\theta}_{ML})}} $$ **convereges in distribution** to $\\mathcal{N}(0,1) $\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise - Asymptotic Consequences of MLE When Choosing The Wrong Model\n",
    "Definitions:\n",
    "* **Kullback-Leibler (KL) Divergence** - defined to be $$ KL(p(x)||q(x)) = \\underset{x \\sim p(x)}{\\mathbb{E}}[\\log \\frac{p(x)}{q(x)}]$$, it is a way to measure \"distance between distribution\" (how far is $q(x)$ from $p(x)$.\n",
    "* **Entropy** - defined to be $$ H(p(X)) = -\\underset{x \\sim p(x)}{\\mathbb{E}}[\\log(p(x))] $$\n",
    "* **The Weak Law of Large Numbers** - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean $$ \\lim_{n \\to \\infty} P(|\\overline{x}_n - \\mu | \\geq \\epsilon) = 0 $$\n",
    "    * In words: as the sample size $n$ grows to infinity, the probability that the sample mean $\\overline{x}_n$ differs from the population mean $\\mu$ by some small amount $ \\epsilon$ is equal to 0.\n",
    "\n",
    "\n",
    "Let the data $\\mathcal{D}$ be drawn i.i.d. from a distribution $p(x)$ which is not necessarily contained in the parametric model (i.e. **there is no $\\theta$ for which $f(x;\\theta) = p(x), \\forall x$**). What is the *upper bound* for the MLE as $N \\to \\infty$? Use the above definitions to guide you and use the *log* of the estimator ($\\log (\\prod_{i=0}^{N-1}f(x_i;\\theta))$)\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "We will use the above definitions to derive another form of the estimator. For every $\\theta$ it holds that:\n",
    "$$ \\log(\\prod_{i=0}^{N-1}f(x_i;\\theta)) = N\\big(\\frac{1}{N}\\sum_{i=0}^{N-1}\\log(f(x_i;\\theta))\\big) $$\n",
    "Using the **weak law of large numbers** as $N \\to \\infty$: $$ \\underset{N \\to \\infty}{\\rightarrow} N \\cdot \\mathbb{E}[\\log(f(x;\\theta))] =  $$ $$ = N \\cdot \\mathbb{E}[\\log(\\frac{f(x;\\theta)p(x)}{p(x)})] $$ $$ =N \\cdot \\mathbb{E}[\\log(\\frac{f(x;\\theta)}{p(x)})] +  N \\cdot \\mathbb{E}[\\log(p(x))] =  $$ $$ -N \\cdot KL(p(x)||f(x; \\theta)) -N \\cdot H(p(x)) $$\n",
    "Thus, the MLE takes the form:  $$ \\lim_{N \\to \\infty} \\theta_{MLE} = \\lim_{N \\to \\infty} argmax_{\\theta} \\log(L(\\theta)) $$ $$ = argmax_{\\theta} \\big( -KL(p(x)||f(x; \\theta)) - H(p(x)) \\big) = argmin_{\\theta} KL(p(x)||f(x; \\theta))  $$ \n",
    "\n",
    "* What is the meaning of this?\n",
    "    * In the limit of **large sample size**, the MLE criterion is equivalent to find the model which is *\"closest\"* (in the KL divergence sense) from the true distribution. That is, when the model cannot be really parameterized with $\\theta$ we are not guaranteed to converge to the true distribution, even for infinite number of samples!\n",
    "    * KL divergence satisfies: $KL(p(x)||q(x)) \\geq 0$, where equality holds $\\iff p(x) = q(x)$. That means that for evey $\\theta$ the likelihood $l(\\theta)$ satisfies $ l(\\theta) \\leq -H(p(x)) $. In the case of model mis-specification and in the limit of large sample size, the log-likelihood estimation cannot reach its maximal value (or the negative log-likelihood cannot reach it minimal value), the **entropy** is the lower bound (to the negative log-likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* Point Estimation - <a href=\"https://www.youtube.com/watch?v=Ieicfj6LYyQ&t=181s\"> MathNStats - Point Estimates </a>\n",
    "* Maximum Likelihood Estimation (MLE)\n",
    "    * Simple Version (6 min) - <a href=\"https://www.youtube.com/watch?v=XepXtl9YKwc\">StatQuest</a>\n",
    "    * Complete Lecture (50 min) - <a href=\"https://www.youtube.com/watch?v=RIawrYLVdIw&t=2263s\">Cornell CS4780</a>\n",
    "* Evaluating Estimators - Bias & MSE - <a href=\"https://www.youtube.com/watch?v=XqWfeND04vs\"> Actuarial Education </a>\n",
    "* Entropy, Cross-Entropy, KL-Divergence - <a href=\"https://www.youtube.com/watch?v=ErfnhcEV1O8\">by Aurélien Géron </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Examples, exercises and definitions from <a href=\"https://probabilitycourse.com/\">Introduction to Probability, Statistics and Random Processes</a> - https://probabilitycourse.com\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
